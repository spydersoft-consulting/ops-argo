syslog:
  enabled: true
  port: 1514
  nodePort: 31050

alloy:
  alloy:
    extraPorts:
      - name: "otlp-grpc"
        port: 4317
        targetPort: 4317
        protocol: "TCP"
      - name: "otlp-http"
        port: 4318
        targetPort: 4318
        protocol: "TCP"
      - name: "tcp-syslog"
        port: 1514
        targetPort: 1514
        protocol: "TCP"
    configMap:
      content: |
        
        remote.kubernetes.secret "vault_app_role_secret" {
          namespace = "cluster-tools"
          name = "k8-app-role-secret"
        }

        remote.vault "targets" {
          server = "https://hcvault.mattgerega.net"
          path = "secrets-k8/monitoring/targets/internal"

          auth.approle {
            role_id = "13f4cd25-b02f-f705-c92d-ef6bfeff1344"
            secret = remote.kubernetes.secret.vault_app_role_secret.data["secret-id"]
          }
        }

        remote.vault "minio" {
          server = "https://hcvault.mattgerega.net"
          path = "secrets-k8/monitoring/minio"

          auth.approle {
            role_id = "13f4cd25-b02f-f705-c92d-ef6bfeff1344"
            secret = remote.kubernetes.secret.vault_app_role_secret.data["secret-id"]
          }
        }

        remote.vault "vault" {
          server = "https://hcvault.mattgerega.net"
          path = "secrets-k8/monitoring/vault"

          auth.approle {
            role_id = "13f4cd25-b02f-f705-c92d-ef6bfeff1344"
            secret = remote.kubernetes.secret.vault_app_role_secret.data["secret-id"]
          }
        }

        remote.vault "garage" {
          server = "https://hcvault.mattgerega.net"
          path = "secrets-k8/monitoring/garage/metrics"

          auth.approle {
            role_id = "13f4cd25-b02f-f705-c92d-ef6bfeff1344"
            secret = remote.kubernetes.secret.vault_app_role_secret.data["secret-id"]
          }
        }

        prometheus.remote_write "default" {
          endpoint {
            url = "http://mimir-gateway.monitoring.svc.cluster.local/api/v1/push"
            headers = {
              "X-Scope-OrgID" = "internal",
            }
          }
          external_labels = {
            cluster = "internal",
            custom_source = "grafana-alloy",
          }
        }

        prometheus.scrape "minio_job" {
          targets = [{
            __address__ = convert.nonsensitive(remote.vault.targets.data["minio-target"]),
          }]
          forward_to   = [prometheus.remote_write.default.receiver]
          job_name     = "minio-job"
          metrics_path = "/minio/v2/metrics/cluster"

          authorization {
            type        = "Bearer"
            credentials = remote.vault.minio.data.authtoken
          }
        }

        prometheus.scrape "vault_job" {
          targets = [{
            __address__ = convert.nonsensitive(remote.vault.targets.data["vault-target"]),
          }]
          forward_to = [prometheus.remote_write.default.receiver]
          job_name   = "vault-job"
          params     = {
            format = ["prometheus"],
          }
          metrics_path = "/v1/sys/metrics"
          scheme       = "https"

          authorization {
            type        = "Bearer"
            credentials = remote.vault.vault.data["metric-token"]
          }
        }

        prometheus.scrape "garage_job" {
          targets = [{
            __address__ = convert.nonsensitive(remote.vault.targets.data["garage-target"]),
          }]
          forward_to   = [prometheus.remote_write.default.receiver]
          job_name     = "garage-job"
          metrics_path = "/metrics"
          scheme       = "http"

          authorization {
            type        = "Bearer"
            credentials = remote.vault.garage.data["metrics-token"]
          }
        }

        prometheus.scrape "unifi_poller_job" {
          targets = [{
            __address__ = "unifi-poller.monitoring.svc.cluster.local:9130",
          }]
          forward_to   = [prometheus.remote_write.default.receiver]
          job_name     = "unifi-poller-job"
          metrics_path = "/metrics"
          scheme       = "http"
        }

        prometheus.scrape "linkerd_gateway_job" {
          targets = [{
            __address__ = "linkerd-gateway.linkerd-multicluster.svc.cluster.local:4191",
          }]
          forward_to   = [prometheus.remote_write.default.receiver]
          job_name     = "linkerd-multicluster-gateway-job"
          metrics_path = "/metrics"
          scheme       = "http"
        }


        loki.write "default" {
          endpoint {
            url = "http://loki-gateway.monitoring.svc.cluster.local/loki/api/v1/push"
            tenant_id = "internal"
          }
          external_labels = {
            cluster = "internal",
            custom_source = "grafana-alloy",
          }
        }

        loki.source.syslog "syslog_receiver" {
          listener {
            address = "0.0.0.0:1514"
            protocol = "tcp"
            idle_timeout = "12h"
            labels = {
              job = "syslog-receiver",
            }
          }
          forward_to = [loki.relabel.syslog.receiver]
        }

        loki.relabel "syslog" {
          forward_to = [loki.write.default.receiver]

          rule {
            source_labels = ["__syslog_message_hostname"]
            target_label  = "host"
          }

          rule {
            source_labels = ["__syslog_message_hostname"]
            target_label  = "hostname"
          }

          rule {
            source_labels = ["__syslog_message_severity"]
            target_label  = "level"
          }

          rule {
            source_labels = ["__syslog_message_app_name"]
            target_label  = "application"
          }

          rule {
            source_labels = ["__syslog_message_facility"]
            target_label  = "facility"
          }

          rule {
            source_labels = ["__syslog_connection_hostname"]
            target_label  = "connection_hostname"
          }
        }

        otelcol.processor.memory_limiter "default" {
          check_interval = "1s"
          limit          = "1GiB"

          output {
            metrics = [otelcol.processor.batch.default.input]
            logs    = [otelcol.processor.batch.default.input]
            traces  = [otelcol.processor.batch.default.input]
          }
        }

        otelcol.processor.batch "default" {
          output {
            metrics = [otelcol.exporter.prometheus.default.input]
            logs    = [otelcol.exporter.loki.default.input]
            traces  = [otelcol.exporter.otlp.default.input]
          }
        }
        otelcol.receiver.otlp "example" {
          grpc {
            endpoint = "0.0.0.0:4317"
          }

          http {
            endpoint = "0.0.0.0:4318"
          }

          output {
            metrics = [otelcol.processor.batch.default.input]
            logs    = [otelcol.processor.batch.default.input]
            traces  = [otelcol.processor.batch.default.input]
          }
        }

        otelcol.exporter.loki "default" {
          forward_to = [loki.write.default.receiver]
        }

        otelcol.exporter.prometheus "default" {
          forward_to = [prometheus.remote_write.default.receiver]
        }

        otelcol.exporter.otlp "default" {
          client {
            endpoint = "tempo-gateway.monitoring.svc.cluster.local:4317"
            headers = {
              "X-Scope-OrgID" = "internal",
            }
            tls {
              insecure = true
            }
          }
        }