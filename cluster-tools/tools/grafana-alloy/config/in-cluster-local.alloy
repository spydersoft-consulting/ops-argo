// =============================================================================
// Vault Configuration
// =============================================================================
// Read AppRole credentials from Kubernetes secret for Vault authentication

remote.kubernetes.secret "vault_app_role_secret" {
  namespace = "cluster-tools"
  name      = "k8-app-role-secret"
}

// Vault remote configuration for monitoring targets
remote.vault "targets" {
  server = "https://hcvault.mattgerega.net"
  path   = "secrets-k8/monitoring/targets/internal"

  auth.approle {
    role_id = "13f4cd25-b02f-f705-c92d-ef6bfeff1344"
    secret  = remote.kubernetes.secret.vault_app_role_secret.data["secret-id"]
  }
}

// Vault remote configuration for MinIO credentials
remote.vault "minio" {
  server = "https://hcvault.mattgerega.net"
  path   = "secrets-k8/monitoring/minio"

  auth.approle {
    role_id = "13f4cd25-b02f-f705-c92d-ef6bfeff1344"
    secret  = remote.kubernetes.secret.vault_app_role_secret.data["secret-id"]
  }
}

// Vault remote configuration for Vault metrics token
remote.vault "vault" {
  server = "https://hcvault.mattgerega.net"
  path   = "secrets-k8/monitoring/vault"

  auth.approle {
    role_id = "13f4cd25-b02f-f705-c92d-ef6bfeff1344"
    secret  = remote.kubernetes.secret.vault_app_role_secret.data["secret-id"]
  }
}

// Vault remote configuration for Garage S3 metrics token
remote.vault "garage" {
  server = "https://hcvault.mattgerega.net"
  path   = "secrets-k8/monitoring/garage/metrics"

  auth.approle {
    role_id = "13f4cd25-b02f-f705-c92d-ef6bfeff1344"
    secret  = remote.kubernetes.secret.vault_app_role_secret.data["secret-id"]
  }
}

// =============================================================================
// Prometheus Configuration
// =============================================================================
// Remote write endpoint for pushing metrics to Mimir

prometheus.remote_write "default" {
  endpoint {
    url = "http://mimir-gateway.monitoring.svc.cluster.local/api/v1/push"
    headers = {
      "X-Scope-OrgID" = "internal",
    }
  }
  external_labels = {
    cluster       = "internal",
    custom_source = "grafana-alloy",
  }
}

// -----------------------------------------------------------------------------
// Prometheus Scrape Jobs
// -----------------------------------------------------------------------------

// Scrape MinIO S3 cluster metrics
prometheus.scrape "minio_job" {
  targets = [{
    __address__ = convert.nonsensitive(remote.vault.targets.data["minio-target"]),
  }]
  forward_to   = [prometheus.remote_write.default.receiver]
  job_name     = "minio-job"
  metrics_path = "/minio/v2/metrics/cluster"

  authorization {
    type        = "Bearer"
    credentials = remote.vault.minio.data.authtoken
  }
}

// Scrape HashiCorp Vault metrics
prometheus.scrape "vault_job" {
  targets = [{
    __address__ = convert.nonsensitive(remote.vault.targets.data["vault-target"]),
  }]
  forward_to   = [prometheus.remote_write.default.receiver]
  job_name     = "vault-job"
  params = {
    format = ["prometheus"],
  }
  metrics_path = "/v1/sys/metrics"
  scheme       = "https"

  authorization {
    type        = "Bearer"
    credentials = remote.vault.vault.data["metric-token"]
  }
}

// Scrape Garage S3 metrics
prometheus.scrape "garage_job" {
  targets = [{
    __address__ = convert.nonsensitive(remote.vault.targets.data["garage-target"]),
  }]
  forward_to   = [prometheus.remote_write.default.receiver]
  job_name     = "garage-job"
  metrics_path = "/metrics"
  scheme       = "http"

  authorization {
    type        = "Bearer"
    credentials = remote.vault.garage.data["metrics-token"]
  }
}

// Scrape Unifi Poller metrics (network monitoring)
prometheus.scrape "unifi_poller_job" {
  targets = [{
    __address__ = "unifi-poller.monitoring.svc.cluster.local:9130",
  }]
  forward_to   = [prometheus.remote_write.default.receiver]
  job_name     = "unifi-poller-job"
  metrics_path = "/metrics"
  scheme       = "http"
}

// Scrape Linkerd multicluster gateway metrics
prometheus.scrape "linkerd_gateway_job" {
  targets = [{
    __address__ = "linkerd-gateway.linkerd-multicluster.svc.cluster.local:4191",
  }]
  forward_to   = [prometheus.remote_write.default.receiver]
  job_name     = "linkerd-multicluster-gateway-job"
  metrics_path = "/metrics"
  scheme       = "http"
}

// =============================================================================
// Loki Configuration
// =============================================================================
// Remote write endpoint for pushing logs to Loki

loki.write "default" {
  endpoint {
    url       = "http://loki-gateway.monitoring.svc.cluster.local/loki/api/v1/push"
    tenant_id = "internal"
  }
  external_labels = {
    cluster       = "internal",
    custom_source = "grafana-alloy",
  }
}

// -----------------------------------------------------------------------------
// Loki Syslog Receiver
// -----------------------------------------------------------------------------
// Receive syslog messages on port 1514 (exposed via NodePort 31050)

// UDP receiver (most common for syslog)
loki.source.syslog "syslog_receiver_udp" {
  listener {
    address      = "0.0.0.0:1514"
    protocol     = "udp"
    labels = {
      job          = "syslog-receiver",
      transport    = "udp",
    }
    // Use RFC 3164 (BSD syslog) format - most network devices use this
    syslog_format = "rfc3164"
  }
  forward_to = [loki.process.extract_hostname.receiver]
}

// TCP receiver (fallback)
loki.source.syslog "syslog_receiver_tcp" {
  listener {
    address      = "0.0.0.0:1515"
    protocol     = "tcp"
    idle_timeout = "12h"
    labels = {
      job          = "syslog-receiver",
      transport    = "tcp",
    }
    // Use RFC 3164 (BSD syslog) format - most network devices use this
    syslog_format = "rfc3164"
  }
  forward_to = [loki.process.extract_hostname.receiver]
}

// Extract hostname from syslog message content using regex
// This is necessary because RFC 3164 internal labels (__syslog_message_hostname)
// are not reliably populated. We extract directly from the log message.
loki.process "extract_hostname" {
  forward_to = [loki.relabel.identify_source.receiver]

  // RFC 3164 format: hostname is the first word in the message
  // Example: "UCG-Max mcad[3252]: teleport.prepare_inform_teleport_json()..."
  stage.regex {
    expression = "^(?P<hostname>[^\\s]+)"
  }

  stage.labels {
    values = {
      hostname = "hostname",
    }
  }
}

// Identify source based on hostname pattern
loki.relabel "identify_source" {
  forward_to = [loki.write.default.receiver]

  // Identify UniFi devices by hostname pattern (UDM, USW, UAP, UCG, UXG)
  rule {
    source_labels = ["hostname"]
    regex         = "(UDM|USW|UAP|UCG|UXG).*"
    target_label  = "source"
    replacement   = "unifi-network"
  }

  rule {
    source_labels = ["hostname"]
    regex         = "(UDM|USW|UAP|UCG|UXG).*"
    target_label  = "source_type"
    replacement   = "network"
  }

  // Identify Synology by hostname pattern
  rule {
    source_labels = ["hostname"]
    regex         = "(cloud|synology).*"
    target_label  = "source"
    replacement   = "synology-nas"
  }

  rule {
    source_labels = ["hostname"]
    regex         = "(cloud|synology).*"
    target_label  = "source_type"
    replacement   = "infrastructure"
  }

  // Default for unknown sources
  rule {
    source_labels = ["source"]
    regex         = "^$"
    target_label  = "source"
    replacement   = "unknown"
  }

  rule {
    source_labels = ["source_type"]
    regex         = "^$"
    target_label  = "source_type"
    replacement   = "unknown"
  }
}

// =============================================================================
// OpenTelemetry Configuration
// =============================================================================
// Receive, process, and export OpenTelemetry metrics, logs, and traces

// -----------------------------------------------------------------------------
// OTLP Receiver
// -----------------------------------------------------------------------------
// Accept OTLP data via gRPC (4317) and HTTP (4318)

otelcol.receiver.otlp "example" {
  grpc {
    endpoint = "0.0.0.0:4317"
  }

  http {
    endpoint = "0.0.0.0:4318"
  }

  output {
    metrics = [otelcol.processor.memory_limiter.default.input]
    logs    = [otelcol.processor.memory_limiter.default.input]
    traces  = [otelcol.processor.memory_limiter.default.input]
  }
}

// -----------------------------------------------------------------------------
// OTLP Processors
// -----------------------------------------------------------------------------

// Limit memory usage to prevent OOM
otelcol.processor.memory_limiter "default" {
  check_interval = "1s"
  limit          = "1GiB"

  output {
    metrics = [otelcol.processor.batch.default.input]
    logs    = [otelcol.processor.batch.default.input]
    traces  = [otelcol.processor.batch.default.input]
  }
}

// Batch telemetry data for efficient export
otelcol.processor.batch "default" {
  output {
    metrics = [otelcol.exporter.prometheus.default.input]
    logs    = [otelcol.exporter.loki.default.input]
    traces  = [otelcol.exporter.otlp.default.input]
  }
}

// -----------------------------------------------------------------------------
// OTLP Exporters
// -----------------------------------------------------------------------------

// Export logs to Loki
otelcol.exporter.loki "default" {
  forward_to = [loki.write.default.receiver]
}

// Export metrics to Prometheus/Mimir
otelcol.exporter.prometheus "default" {
  forward_to = [prometheus.remote_write.default.receiver]
}

// Export traces to Tempo
otelcol.exporter.otlp "default" {
  client {
    endpoint = "tempo-gateway.monitoring.svc.cluster.local:4317"
    headers = {
      "X-Scope-OrgID" = "internal",
    }
    tls {
      insecure = true
    }
  }
}
