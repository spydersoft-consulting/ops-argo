// =============================================================================
// Vault Configuration
// =============================================================================
// Read AppRole credentials from Kubernetes secret for Vault authentication

remote.kubernetes.secret "vault_app_role_secret" {
  namespace = "cluster-tools"
  name      = "k8-app-role-secret"
}

// Vault remote configuration for monitoring targets
remote.vault "targets" {
  server = "https://hcvault.mattgerega.net"
  path   = "secrets-k8/monitoring/targets/internal"

  auth.approle {
    role_id = "13f4cd25-b02f-f705-c92d-ef6bfeff1344"
    secret  = remote.kubernetes.secret.vault_app_role_secret.data["secret-id"]
  }
}

// Vault remote configuration for Vault metrics token
remote.vault "vault" {
  server = "https://hcvault.mattgerega.net"
  path   = "secrets-k8/monitoring/vault"

  auth.approle {
    role_id = "13f4cd25-b02f-f705-c92d-ef6bfeff1344"
    secret  = remote.kubernetes.secret.vault_app_role_secret.data["secret-id"]
  }
}

// Vault remote configuration for Garage S3 metrics token
remote.vault "garage" {
  server = "https://hcvault.mattgerega.net"
  path   = "secrets-k8/monitoring/garage/metrics"

  auth.approle {
    role_id = "13f4cd25-b02f-f705-c92d-ef6bfeff1344"
    secret  = remote.kubernetes.secret.vault_app_role_secret.data["secret-id"]
  }
}

// =============================================================================
// Prometheus Configuration
// =============================================================================
// Remote write endpoint for pushing metrics to Mimir

prometheus.remote_write "default" {
  endpoint {
    url = "http://mimir-gateway.monitoring.svc.cluster.local/api/v1/push"
    headers = {
      "X-Scope-OrgID" = "internal",
    }
  }
  external_labels = {
    cluster       = "internal",
    custom_source = "grafana-alloy",
  }
}

// -----------------------------------------------------------------------------
// Prometheus Scrape Jobs
// -----------------------------------------------------------------------------

// Scrape HashiCorp Vault metrics
prometheus.scrape "vault_job" {
  targets = [{
    __address__ = convert.nonsensitive(remote.vault.targets.data["vault-target"]),
  }]
  forward_to   = [prometheus.remote_write.default.receiver]
  job_name     = "vault-job"
  params = {
    format = ["prometheus"],
  }
  metrics_path = "/v1/sys/metrics"
  scheme       = "https"

  authorization {
    type        = "Bearer"
    credentials = remote.vault.vault.data["metric-token"]
  }
}

// Scrape Garage S3 metrics
prometheus.scrape "garage_job" {
  targets = [{
    __address__ = convert.nonsensitive(remote.vault.targets.data["garage-target"]),
  }]
  forward_to   = [prometheus.remote_write.default.receiver]
  job_name     = "garage-job"
  metrics_path = "/metrics"
  scheme       = "http"

  authorization {
    type        = "Bearer"
    credentials = remote.vault.garage.data["metrics-token"]
  }
}

// Scrape Unifi Poller metrics (network monitoring)
//prometheus.scrape "unifi_poller_job" {
//  targets = [{
//    __address__ = "unifi-poller.monitoring.svc.cluster.local:9130",
//  }]
//  forward_to   = [prometheus.remote_write.default.receiver]
//  job_name     = "unifi-poller-job"
//  metrics_path = "/metrics"
//  scheme       = "http"
//}

// Scrape Linkerd multicluster gateway metrics
prometheus.scrape "linkerd_gateway_job" {
  targets = [{
    __address__ = "linkerd-gateway.linkerd-multicluster.svc.cluster.local:4191",
  }]
  forward_to   = [prometheus.remote_write.default.receiver]
  job_name     = "linkerd-multicluster-gateway-job"
  metrics_path = "/metrics"
  scheme       = "http"
}

// =============================================================================
// Loki Configuration
// =============================================================================
// Remote write endpoint for pushing logs to Loki

loki.write "default" {
  endpoint {
    url       = "http://loki-gateway.monitoring.svc.cluster.local/loki/api/v1/push"
    tenant_id = "internal"
  }
  external_labels = {
    cluster       = "internal",
    custom_source = "grafana-alloy",
  }
}

// -----------------------------------------------------------------------------
// Loki Syslog Receiver
// -----------------------------------------------------------------------------
// Receive syslog messages on port 1514 (exposed via NodePort 31050)

// UDP receiver (most common for syslog)
loki.source.syslog "syslog_receiver_udp" {
  listener {
    address      = "0.0.0.0:1514"
    protocol     = "udp"
    labels = {
      job          = "syslog-receiver",
      transport    = "udp",
    }
    // Use RFC 3164 (BSD syslog) format - most network devices use this
    syslog_format = "rfc3164"
  }
  forward_to = [loki.process.extract_hostname.receiver]
}

// TCP receiver (fallback)
loki.source.syslog "syslog_receiver_tcp" {
  listener {
    address      = "0.0.0.0:1515"
    protocol     = "tcp"
    idle_timeout = "12h"
    labels = {
      job          = "syslog-receiver",
      transport    = "tcp",
    }
    // Use RFC 3164 (BSD syslog) format - most network devices use this
    syslog_format = "rfc3164"
  }
  forward_to = [loki.process.extract_hostname.receiver]
}

// Extract hostname from syslog message content using regex
// This is necessary because RFC 3164 internal labels (__syslog_message_hostname)
// are not reliably populated. We extract directly from the log message.
loki.process "extract_hostname" {
  forward_to = [loki.relabel.identify_source.receiver]

  // RFC 3164 format: hostname is the first word in the message
  // Example: "UCG-Max mcad[3252]: teleport.prepare_inform_teleport_json()..."
  stage.regex {
    expression = "^(?P<hostname>[^\\s]+)"
  }

  stage.labels {
    values = {
      hostname = "hostname",
    }
  }
}

// Identify source based on hostname pattern
loki.relabel "identify_source" {
  forward_to = [loki.write.default.receiver]

  // Identify UniFi devices by hostname pattern (UDM, USW, UAP, UCG, UXG)
  rule {
    source_labels = ["hostname"]
    regex         = "(UDM|USW|UAP|UCG|UXG).*"
    target_label  = "source"
    replacement   = "unifi-network"
  }

  rule {
    source_labels = ["hostname"]
    regex         = "(UDM|USW|UAP|UCG|UXG).*"
    target_label  = "source_type"
    replacement   = "network"
  }

  // Identify Synology by hostname pattern
  rule {
    source_labels = ["hostname"]
    regex         = "(cloud|synology).*"
    target_label  = "source"
    replacement   = "synology-nas"
  }

  rule {
    source_labels = ["hostname"]
    regex         = "(cloud|synology).*"
    target_label  = "source_type"
    replacement   = "infrastructure"
  }

  // Default for unknown sources
  rule {
    source_labels = ["source"]
    regex         = "^$"
    target_label  = "source"
    replacement   = "unknown"
  }

  rule {
    source_labels = ["source_type"]
    regex         = "^$"
    target_label  = "source_type"
    replacement   = "unknown"
  }
}

// -------------------------------------------------------------------
// METRICS DISCOVERY - Custom Annotations
// -------------------------------------------------------------------

// Discover services using spydersoft.io annotations
discovery.kubernetes "services" {
  role = "service"
}

discovery.relabel "spydersoft_metrics" {
  targets = discovery.kubernetes.services.targets
  
  // Use YOUR custom annotation namespace
  // metrics.spydersoft.io/scrape instead of prometheus.io/scrape
  rule {
    source_labels = ["__meta_kubernetes_service_annotation_metrics_spydersoft_io_scrape"]
    regex         = "true"
    action        = "keep"
  }
  
  // Get port from metrics.spydersoft.io/port
  rule {
    source_labels = ["__address__", "__meta_kubernetes_service_annotation_metrics_spydersoft_io_port"]
    regex         = "([^:]+)(?::\\d+)?;(\\d+)"
    target_label  = "__address__"
    replacement   = "$1:$2"
    action        = "replace"
  }
  
  // Get path from metrics.spydersoft.io/path (default /metrics)
  rule {
    source_labels = ["__meta_kubernetes_service_annotation_metrics_spydersoft_io_path"]
    regex         = "(.+)"
    target_label  = "__metrics_path__"
    action        = "replace"
  }
  
  // Get scheme from metrics.spydersoft.io/scheme
  rule {
    source_labels = ["__meta_kubernetes_service_annotation_metrics_spydersoft_io_scheme"]
    regex         = "(https?)"
    target_label  = "__scheme__"
    action        = "replace"
  }
  
  // Get query parameters from metrics.spydersoft.io/param_<name>
  // For SNMP exporters: metrics.spydersoft.io/param_module and metrics.spydersoft.io/param_target
  rule {
    source_labels = ["__meta_kubernetes_service_annotation_metrics_spydersoft_io_param_module"]
    regex         = "(.+)"
    target_label  = "__param_module"
    action        = "replace"
  }
  
  rule {
    source_labels = ["__meta_kubernetes_service_annotation_metrics_spydersoft_io_param_target"]
    regex         = "(.+)"
    target_label  = "__param_target"
    action        = "replace"
  }
  
  rule {
    source_labels = ["__meta_kubernetes_service_annotation_metrics_spydersoft_io_param_auth"]
    regex         = "(.+)"
    target_label  = "__param_auth"
    action        = "replace"
  }
  
  // Add standard labels
  rule {
    source_labels = ["__meta_kubernetes_namespace"]
    target_label  = "namespace"
  }
  
  rule {
    source_labels = ["__meta_kubernetes_service_name"]
    target_label  = "service"
  }
  
  rule {
    source_labels = ["__meta_kubernetes_service_name"]
    target_label  = "job"
  }
}

prometheus.scrape "spydersoft_metrics" {
  targets    = discovery.relabel.spydersoft_metrics.output
  forward_to = [prometheus.remote_write.default.receiver]
  
  honor_labels     = true
  honor_timestamps = true
}

// -------------------------------------------------------------------
// LOGS DISCOVERY - Custom Annotations
// -------------------------------------------------------------------

// Discover pods/services for log collection
discovery.kubernetes "log_sources" {
  role = "pod"  // Use pod role for log collection
}

discovery.relabel "spydersoft_logs" {
  targets = discovery.kubernetes.log_sources.targets
  
  // Use logs.spydersoft.io/collect annotation
  rule {
    source_labels = ["__meta_kubernetes_pod_annotation_logs_spydersoft_io_collect"]
    regex         = "true"
    action        = "keep"
  }
  
  // Get log path from annotation (for file-based logs)
  rule {
    source_labels = ["__meta_kubernetes_pod_annotation_logs_spydersoft_io_path"]
    regex         = "(.+)"
    target_label  = "__path__"
    action        = "replace"
  }
  
  // Default log path if not specified (stdout)
  rule {
    source_labels = ["__path__"]
    regex         = ""
    replacement   = "/var/log/pods/*$1/*.log"
    target_label  = "__path__"
  }
  
  // Add pod metadata labels
  rule {
    source_labels = ["__meta_kubernetes_namespace"]
    target_label  = "namespace"
  }
  
  rule {
    source_labels = ["__meta_kubernetes_pod_name"]
    target_label  = "pod"
  }
  
  rule {
    source_labels = ["__meta_kubernetes_pod_container_name"]
    target_label  = "container"
  }
  
  rule {
    source_labels = ["__meta_kubernetes_pod_node_name"]
    target_label  = "node"
  }
  
  // Get log format from annotation (json, logfmt, text)
  rule {
    source_labels = ["__meta_kubernetes_pod_annotation_logs_spydersoft_io_format"]
    target_label  = "__log_format__"
  }
  
  // Copy all pod labels
  rule {
    regex       = "__meta_kubernetes_pod_label_(.+)"
    action      = "labelmap"
    replacement = "$1"
  }
}

// Collect logs from discovered pods
loki.source.kubernetes "spydersoft_logs" {
  targets    = discovery.relabel.spydersoft_logs.output
  forward_to = [loki.process.parse_logs.receiver]
}

// Process logs based on format
loki.process "parse_logs" {
  forward_to = [loki.write.default.receiver]
  
  // Parse JSON logs
  stage.match {
    selector = "{__log_format__=\"json\"}"
    stage.json {
      expressions = {
        level   = "level",
        message = "msg",
      }
    }
  }
  
  // Parse logfmt logs
  stage.match {
    selector = "{__log_format__=\"logfmt\"}"
    stage.logfmt {
      mapping = {
        level = "",
        msg   = "",
      }
    }
  }
  
  // Extract log level and set as label
  stage.labels {
    values = {
      level = "",
    }
  }
}


// =============================================================================
// OpenTelemetry Configuration
// =============================================================================
// Receive, process, and export OpenTelemetry metrics, logs, and traces

// -----------------------------------------------------------------------------
// OTLP Receiver
// -----------------------------------------------------------------------------
// Accept OTLP data via gRPC (4317) and HTTP (4318)

otelcol.receiver.otlp "example" {
  grpc {
    endpoint = "0.0.0.0:4317"
  }

  http {
    endpoint = "0.0.0.0:4318"
  }

  output {
    metrics = [otelcol.processor.memory_limiter.default.input]
    logs    = [otelcol.processor.memory_limiter.default.input]
    traces  = [otelcol.processor.memory_limiter.default.input]
  }
}

// -----------------------------------------------------------------------------
// OTLP Processors
// -----------------------------------------------------------------------------

// Limit memory usage to prevent OOM
otelcol.processor.memory_limiter "default" {
  check_interval = "1s"
  limit          = "1GiB"

  output {
    metrics = [otelcol.processor.batch.default.input]
    logs    = [otelcol.processor.batch.default.input]
    traces  = [otelcol.processor.batch.default.input]
  }
}

// Batch telemetry data for efficient export
otelcol.processor.batch "default" {
  output {
    metrics = [otelcol.exporter.prometheus.default.input]
    logs    = [otelcol.exporter.loki.default.input]
    traces  = [otelcol.exporter.otlp.default.input]
  }
}

// -----------------------------------------------------------------------------
// OTLP Exporters
// -----------------------------------------------------------------------------

// Export logs to Loki
otelcol.exporter.loki "default" {
  forward_to = [loki.write.default.receiver]
}

// Export metrics to Prometheus/Mimir
otelcol.exporter.prometheus "default" {
  forward_to = [prometheus.remote_write.default.receiver]
}

// Export traces to Tempo
otelcol.exporter.otlp "default" {
  client {
    endpoint = "tempo-gateway.monitoring.svc.cluster.local:4317"
    headers = {
      "X-Scope-OrgID" = "internal",
    }
    tls {
      insecure = true
    }
  }
}
